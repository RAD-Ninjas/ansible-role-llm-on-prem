---
# vars file for ansible-role-llm-on-prem

# AI Tool Keys.
# DO NOT put actual keys here. This example uses 1password by default.
ai_huggingface_token: "{{ lookup('community.general.onepassword', 'home-network-huggingface-read-token', field='password', vault='HomeAutomation') }}"
ai_openai_api_key: "{{ lookup('community.general.onepassword', 'home-network-openai-api-key', field='password', vault='HomeAutomation') }}"

# The Path of the LLM model in huggingface.
# Vicuna is open and safe to test with. You can also use meta-llama/Llama-2-7b-chat-hf 
# if you have access to it.
llm_model_path: lmsys/vicuna-7b-v1.3
# Tha name of the model in huggingface.
llm_model_name: vicuna-7b-v1.3

# These can be cpu, cuda, metal (for Mac), etc
ai_profile: cpu

# Number of CPU threads to use for CPU inference
ai_cpu_threads: 5

ai_openvino_ir_path: /home/openvino/models/llama-2/ir_model
ai_openvino_device: GPU.1

# The user to become when running docker commands and instaling the inference
ai_user: "docker"
ai_group: "docker"

ai_super_user: root

# If you want to install a tag or specific version, use this. Otherwise it will just go to the latest main checkin.
llm_on_prem_version: "main"

llm_on_prem_template_dir: ../templates
llm_on_prem_home_dir: "/home/{{ ai_user }}"
llm_on_prem_home: "{{ llm_on_prem_home_dir }}/llm-on-prem"
llm_on_prem_url: https://github.com/RAD-Ninjas/llm-on-prem
llm_on_prem_install_script: "{{llm_on_prem_home}}/models/scripts/start-mac-worker.sh"
llm_on_prem_dockerfile: "{{llm_on_prem_home}}/docker-compose.yml"
# Home for the python virtual environment the inference workers will use
llm_on_prem_venv_home: "{{ llm_on_prem_home }}/venv"
huggingface_cache_dir: "{{ llm_on_prem_home_dir }}/.cache/huggingface"

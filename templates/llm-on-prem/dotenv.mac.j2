#
# PUBLIC ENVIRONMENT VARIABLES, EXPOSED TO THE BROWSER
#
export NEXT_PUBLIC_PANGEA_DOMAIN="$NEXT_PUBLIC_PANGEA_DOMAIN$"
export NEXT_PUBLIC_AUTHN_CLIENT_TOKEN="$NEXT_PUBLIC_AUTHN_CLIENT_TOKEN$"
export NEXT_PUBLIC_AUTHN_HOSTED_LOGIN_URL="$NEXT_PUBLIC_AUTHN_HOSTED_LOGIN_URL$"

#
# PRIVATE ENVIRONMENT VARIABLES, NOT EXPOSED TO THE BROWSER
# ONLY AVAILABLE ON THE SERVER
#
export PANGEA_SERVICE_TOKEN="$PANGEA_SERVICE_TOKEN$"
export OPENAI_API_KEY="{{ ai_openai_api_key }}"
export OPTIONS_REDACT_USER_PROMPTS="true"
export OPTIONS_AUDIT_USER_PROMPTS="true"
export OPTIONS_THREAT_ANALYSE_SERVICE_RESPONSES="true"

# Replace with your huggingface token
export HF_TOKEN="{{ ai_huggingface_token }}"

# Keep this the same if running locally
export MODEL_BASE_PATH="http://${HOSTNAME}:8000/v1"

# LLM Runtime Settings =============================

# Model Path
# export MODEL_PATH="meta-llama/Llama-2-7b-chat-hf"

# HuggingFace Model Name
# export MODEL_NAME="Llama-2-7b-chat-hf"

# Model Path
export MODEL_PATH="{{ ai_model_path }}"

# HuggingFace Model Name
export MODEL_NAME="{{ ai_model_name }}"

# Number of CPU threads to use for CPU inference
export CPU_THREADS={{ ai_cpu_threads }}

# OpenVINO Settings
export IR_PATH={{ ai_openvino_ir_path }}
export DEVICE="{{ ai_openvino_device }}"

# Pytorch config
export OM_NUM_THREADS=8
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
export OPENBLAS_NUM_THREADS=8
export VECLIB_MAXIMUM_THREADS=8
export NUMEXPR_NUM_THREADS=8

